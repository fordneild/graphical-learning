{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing LDA Clusters\n",
    "\n",
    "It can be challenging to evaluate a model trained with Unsupervised Learning. There are no ground truth labels. The learned topics aren't \"right\" or \"wrong\". The questions is, are the topics useful? \n",
    "\n",
    "In this notebook you will visualize the results from your topic modeling in several different ways and reflect on how you can evaluate an Unsupervised Learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' import project files '''\n",
    "from data import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./datasets/\"\n",
    "# note: you can change the dataset here\n",
    "dataset = \"cb/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/cb/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b3ea2567cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/homework5/data.py\u001b[0m in \u001b[0;36mbuild_dtm\u001b[0;34m(datadir, num_docs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'common_words.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'common_words.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_docs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/cb/'"
     ]
    }
   ],
   "source": [
    "X_sparse, vocab = build_dtm(data_dir+dataset, num_docs=100)\n",
    "X = X_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started: Understanding the Data\n",
    "Throughout this course, we have emphasized that an important step in ML algorithm development is understanding your data. We have provided the document-word matrix $X$. $X$ is of size num-documents by num-words. You can verify this by looking at the length of vocab and how many documents are in the dataset folder you selected. The entries in $X$ must be integers, element $X[i, j]$ is the count of how many times word $j$ occured in document $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.imshow(X[0:100, 0:100], vmin=0, vmax=3)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize a small portion of $X$. Note that most entries are zero, indicating that the specified vocab word is absent from that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What words occur most frequently in our dataset? \n",
    "To find the total number of occurances of each word in the dataset, we sum each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.sum(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words   = np.sort(word_counts)\n",
    "sorted_indices = np.argsort(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = len(vocab)\n",
    "''' Visualize the 10 most frequently used words in our dataset '''\n",
    "fig = plt.figure()\n",
    "plt.bar(range(10), sorted_words[W-10:W])\n",
    "plt.xticks(range(10), list(vocab[sorted_indices[W-10:W]]), rotation=30)\n",
    "plt.ylabel('Word Count')\n",
    "plt.xlabel('Word')\n",
    "plt.title(\"Most Common Words in the \" + dataset +\" Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Your Results\n",
    "In this section we have provided code to visualize the learned topics in several ways. At the end of the notebook, you will reflect on your model and describe which visualization techniques were helpful and which were not. We have included text throughout to help walk you through the different visualizations. <br>\n",
    "<b>The questions you need to answer for credit are at the very end and are labeled 'Evaluation Questions'.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' running our code '''\n",
    "inference_type  = 'gibbs_sampling' # or 'sum_product'\n",
    "num_topics = 10\n",
    "alpha = 0.1\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the appropriate model\n",
    "if inference_type == 'gibbs_sampling':\n",
    "    inference = GibbsSampling(num_topics=num_topics, num_docs=X.shape[0], \n",
    "        num_words=X.shape[1], alpha=alpha, beta=beta)\n",
    "elif inference_type == 'sum_product':\n",
    "    inference = SumProduct(num_topics=num_topics, num_docs=X.shape[0], \n",
    "        num_words=X.shape[1], num_nonzero=X_sparse.nnz, alpha=alpha, beta=beta)\n",
    "else:\n",
    "    raise Exception('The model given by --model is not yet supported.')\n",
    "\n",
    "# Run the training loop\n",
    "model = LDA(inference=inference)\n",
    "model.fit(X=X_sparse, iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a plot of the negative log likelihood of your model over the training iterations. The likelihood should be increasing over the iterations. It should be converging by your final iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(model.inference.loglikelihoods)\n",
    "plt.title('Negative Log Likelihood')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "Here we visualize the LDA score given the specified topic for the 10 most frequently used words in our dataset. The higher the score, the more likely the word is in the given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W = len(vocab)\n",
    "fig = plt.figure()\n",
    "for i in range(model.inference.num_topics):\n",
    "\n",
    "    plt.scatter(range(10), model.inference.nzw[i][sorted_indices[W-10:W]]/(model.inference.nz[i]/np.sum(model.inference.nz)), alpha=0.4, label=\"Topic \"+ str(i))  \n",
    "    \n",
    "plt.xticks(range(10), list(vocab[sorted_indices[W-10:W]]), rotation=30)\n",
    "plt.ylabel('Word Score given Topic')\n",
    "plt.xlabel('Word')\n",
    "plt.title(\"Most Common Words in the \" + dataset +\" Dataset\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "Here we print out the top $K$ words associated with each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "model.predict(vocab=vocab, K=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we plot the top $K$ words by their score given the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W = len(vocab)\n",
    "for i in range(model.inference.num_topics):\n",
    "    topic_words   = np.sort(model.inference.nzw[i]/(model.inference.nz[i]/np.sum(model.inference.nz)))\n",
    "    topic_indices = np.argsort(model.inference.nzw[i])\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.bar(range(K), topic_words[W-K:W])\n",
    "    plt.xticks(range(K), list(vocab[topic_indices[W-K:W]]), rotation=30)\n",
    "    plt.ylabel('Word Score given Topic')\n",
    "    plt.xlabel('Word')\n",
    "    plt.title(\"Most Common Words in Topic \" + str(i))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "Finally, we generate word clouds for the topics. The word size is related to the word score given the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=None,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=K,\n",
    "                  colormap='tab10',\n",
    "                  prefer_horizontal=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dicts = []\n",
    "for i in range(model.inference.num_topics):\n",
    "    t_dict = {}\n",
    "    for j in range(len(vocab)):\n",
    "        t_dict[vocab[j]] = model.inference.nzw[i][j]/(model.inference.nz[i]/np.sum(model.inference.nz))\n",
    "        \n",
    "    topic_dicts.append(t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(model.inference.num_topics):\n",
    "    fig = plt.figure()\n",
    "    plt.gca().imshow(cloud.generate_from_frequencies(topic_dicts[i]))\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Questions\n",
    "* In the introduction, we stated the data for this assignment comes from American political blogs. When do you think these blog posts are from? <br>\n",
    "\n",
    "<i> Your answer goes here </i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the code for one dataset. Which dataset did you choose? Do you see any patterns to describe the different topics? Compare the topics you observe running with 'gibbs_sampling' with 'sum_product'. Are there any salient differences? <br>\n",
    "\n",
    "<i> Your answer goes here. </i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the remaining questions, you can answer using only one inference method. Note, Sum Product tends to run faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiment running with a different number of topics. What changes with the different number of topics? How would you go about selecting the number of topics to use in practice? <br> \n",
    "\n",
    "<i> Your answer goes here. </i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now run the visualization code for at least 2 other datasets. Which datasets did you choose? Do you notice different patterns? Can you guess the point of view in the different datasets?\n",
    "\n",
    "<i> Your answer goes here. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which visualizations did you find useful? Which were not useful? What task can you imagine this topic model being used for? Can you identify a short-coming of your current model? \n",
    "\n",
    "<i> Your answer goes here. </i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
